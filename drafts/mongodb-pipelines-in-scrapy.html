<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="Aly Sivji" />
    <meta name="generator" content="Pelican (VoidyBootstrap theme)" />

    <title>Scraping Websites into MongoDB using Scrapy Pipelines - Siv Scripts</title>

   
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
              integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
              crossorigin="anonymous" />
      <link rel="stylesheet"
            href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
            integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN"
            crossorigin="anonymous">


      <link rel="stylesheet" href="/theme/css/tango.css" />
      <link rel="stylesheet" href="/theme/css/voidybootstrap.css" />
      <link rel="stylesheet" href="/theme/css/dataframe.css" />

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js" integrity="sha384-FFgGfda92tXC8nCNOxrCQ3R8x1TNkMFqDZVQdDaaJiiVbjkPBXIJBx0o7ETjy8Bh" crossorigin="anonymous"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js" integrity="sha384-ZoaMbDF+4LeFxg6WdScQ9nnR1QC2MIRxA1O9KWEXQwns1G8UNyIEZIQidzb0T1fo" crossorigin="anonymous"></script>
    <![endif]-->

    <link rel="shortcut icon" href="/favicon.ico" />
  </head>

  <body>
   
    <nav class="navbar navbar-default">
      <div class="container">
	   <div class="navbar-header">
		<button type="button" class="navbar-toggle" 
				data-toggle="collapse" data-target="#main-navbar-collapse">
		  <span class="sr-only">Toggle navigation</span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		</button>
		<a class="navbar-brand" href="/" rel="home">
          <i class="fa fa-home fa-fw fa-lg"> </i> </a>
       </div>

      <div class="collapse navbar-collapse" id="main-navbar-collapse">
        <ul class="nav navbar-nav">
              <li>
                <a href="/pages/about-me.html">About</a>
              </li>
            <li class="divider"></li>
            <li class="">
              <a href="/archives.html">Archives</a>
            </li>
          <li class="divider"></li>
        </ul> <!-- /nav -->
      </div> <!-- /navbar-collapse -->
	  </div> <!-- /container -->
    </nav> <!-- /navbar -->

	<div class="jumbotron" id="overview">
	  <div class="container">
		<h1><a href="/">Siv Scripts</a></h1>
		<p class="lead">Solving Problems Using Code</p>
	  </div>
	</div>

    <div class="container" id="main-container">
      <div class="row">
        <div class="col-md-9" id="content">
<article itemscope="itemscope" itemtype="http://schema.org/BlogPosting">
  <header class="article-header">
<abbr class="article-header-date">
  Wed 01 February 2017
</abbr> <h1>
  <a href="/drafts/mongodb-pipelines-in-scrapy.html" rel="bookmark"
     title="Permalink to Scraping Websites into MongoDB using Scrapy Pipelines">
    Scraping Websites into MongoDB using Scrapy Pipelines
  </a>
</h1><div class="article-header-info">
  <p>
      Posted by <a href="/author/aly-sivji.html">Aly Sivji</a>
    in 
    <a href="/category/tutorials.html">
      Tutorials</a>
    &nbsp;&nbsp;
  </p>
</div> <!-- /.article-header-info -->  </header>
  <div class="content-body" itemprop="text articleBody">
	<blockquote>
<h2>Bullet Points</h2>
<ul>
<li>Discuss advantages of using Scrapy framework</li>
<li>Create Reddit spider and scrape top posts from list of subreddits</li>
<li>Implement Scrapy pipeline to send scraped data into MongoDB</li>
</ul>
</blockquote>
<hr>
<h2>Introduction</h2>
<p>Wouldn't it be great if every website had a <em>free</em> API we could poll to get the data we wanted?</p>
<p>Sure we can hack together something primative using <a href="http://docs.python-requests.org/en/master/">Requests</a> and <a href="https://www.crummy.com/software/BeautifulSoup/">Beautiful Soup (bs4)</a>, but if you want to do anything more complex like getting the next page or creating data validation pipelines, you're going to have to do a lot of work.</p>
<p>There is where <a href="https://scrapy.org/">Scrapy</a> shines. It provides an extendible web scraping framework that you can use in any project requiring extraction of structured data.</p>
<p>I highly recommend the reader work through <a href="https://doc.scrapy.org/en/master/intro/tutorial.html">the Scrapy tutorial from the documentation</a>. It's a great introduction into the terminology and process flow of the framework. This tutorial is written on the assumption that the reader is familiar with Scrapy.</p>
<p>Acknowledgements: I used <a href="https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/">this Real Python post</a> as a guide along with the <a href="https://doc.scrapy.org/en/1.3/">latest version of Scrapy docs (v1.3)</a>.</p>
<hr>
<h2>Project Description</h2>
<p><em>Author Note: Always read the website's robot.txt file before writing a scraper. Be nice when making requests. Be nice in general. Good rule to live by.</em></p>
<p>Reddit provides a platform for niche communities to have discussions on very specific topics. I frequent <a href="http://www.reddit.com/r/peloton">/r/peloton</a>, <a href="http://www.reddit.com/r/datascience">/r/datascience</a>, and <a href="http://www.reddit.com/r/python">/r/python</a> (amongst others) to get a quick digest of what's going on in the communities I'm a part of. The Reddit voting system, along with the tendency of people to correct others, ensures that I won't have to spend a lot of time wading through filler to get the information I am looking for.</p>
<p>If I can design and then automate a process that can check a list of subreddits and send me a weekly digest, I can stop wasting my time on Reddit and spend more time working on projects like this.</p>
<blockquote>
<h3>Process Outline</h3>
<ul>
<li>Given a list of subredits, scrape 'Top Posts'</li>
<li>Build pipeline to store item in MongoDB</li>
<li>Automate scaping and step up as CRON job or trigger function on Bluemix</li>
<li>Design website to show scraped data stored in MongoDB<ul>
<li>Research into Angular2 vs Django/Flask</li>
</ul>
</li>
</ul>
</blockquote>
<p>This will be the first in a series of posts that will create a fullstack solution to the workflow described above.</p>
<hr>
<h2>What You Need to Follow Along</h2>
<h3>Development Tools (Stack)</h3>
<ul>
<li><a href="https://www.python.org/downloads/">Python 3</a></li>
<li><a href="https://doc.scrapy.org/en/latest/intro/install.html">Scrapy 1.3</a></li>
<li><a href="https://docs.mongodb.com/manual/installation/">MongoDB</a><ul>
<li>Installed locally or use a service like <a href="https://mlab.com/">MLab</a></li>
<li>I will be using a local instance, but you can follow along with <a href="https://docs.mongodb.com/manual/reference/connection-string/">any Mongo URI</a></li>
</ul>
</li>
<li><a href="http://api.mongodb.com/python/current/installation.html">PyMongo</a></li>
</ul>
<h3>Code</h3>
<ul>
<li><a href="https://github.com/alysivji/reddit-top-posts-scrapy">Github Repo</a></li>
</ul>
<hr>
<h2>Create Scrapy Project</h2>
<p>Create a project for the folder and navigate to it in the terminal. We will use Scrapy's <a href="https://doc.scrapy.org/en/latest/topics/commands.html">command line tools</a> to initialize our project.</p>
<div class="highlight"><pre><span></span><span class="go">scrapy startproject reddit</span>
<span class="go">cd reddit</span>
</pre></div>


<p>The directory structure should look as follows:</p>
<div class="highlight"><pre><span></span><span class="go">.</span>
<span class="go">├── scrapy.cfg              # deploy configuration file</span>
<span class="go">└── reddit                  # project&#39;s Python module</span>
<span class="go">    ├── __init__.py</span>
<span class="go">    ├── items.py            # items definition file</span>
<span class="go">    ├── middlewares.py</span>
<span class="go">    ├── pipelines.py        # project pipelines file</span>
<span class="go">    ├── settings.py         # project settings</span>
<span class="go">    └── spiders             # dir to store spiders</span>
<span class="go">        └── __init__.py</span>
</pre></div>


<p><a href="https://doc.scrapy.org/en/latest/intro/tutorial.html#creating-a-project">File descriptions from Scrapy docs</a></p>
<hr>
<h2>Implement Crawler</h2>
<h3>Items to Scrape</h3>
<p>Let's take a look at the <a href="https://www.reddit.com/r/peloton/top/?sort=top&amp;t=all">all-time list of top posts from /r/peloton</a>. Below is a screenshot with the items we want to scrape highlighed.</p>
<p><img src="..\images\2_reddit_items_to_parse.png" alt="Reddit Top Posts -- items to scrape" width=750/></p>
<p>From this we can create our item templates. I added a few more pieces of information (subreddit, date) to provide additional context.</p>
<div class="highlight"><pre><span></span><span class="c1"># items.py</span>

<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">RedditItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Defining the storage containers for the data we</span>
<span class="sd">    plan to scrape</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">sub</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">commentsUrl</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>


<h3>Create Spider</h3>
<p>We will follow the <a href="https://doc.scrapy.org/en/latest/intro/tutorial.html">template from the Scrapy docs</a> to create the spider.</p>
<p>The first part of the code defines the spider settings and then tells Scrapy which URLs to parse (<code>start_urls</code> variable). <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions">Shoutout to list comprehensions</a>!</p>
<p>The parse function tells Scrapy how to process each of the downloaded reponses (<a href="https://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse">docs</a>). I've done a bit of front-end web development in a previous life so I decided to use CSS selectors to extract items from the HTML source (<a href="https://doc.scrapy.org/en/latest/topics/selectors.html">more details in the Scrapy docs</a>). We also use <a href="https://wiki.python.org/moin/Generators">generators</a> since we're all about being Pythonic on this blog. And because the Scrapy documentation uses generators. If you're curious about Python generators, I highly recommend <a href="https://www.youtube.com/watch?v=5-qadlG7tWo">this awesome David Beazley talk from PyCon 2014</a>.</p>
<div class="highlight"><pre><span></span><span class="c1"># spiders/__init__.py</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span> <span class="k">as</span> <span class="n">dt</span>
<span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">reddit.items</span> <span class="kn">import</span> <span class="n">RedditItem</span>

<span class="k">class</span> <span class="nc">PostSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;post&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reddit.com&#39;</span><span class="p">]</span>

    <span class="n">reddit_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;datascience&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;python&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;programming&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;machinelearning&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># list comprehension</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.reddit.com/r/&#39;</span> <span class="o">+</span> <span class="n">sub</span> <span class="o">+</span> <span class="s1">&#39;/top/?sort=top&amp;t=&#39;</span> <span class="o">+</span> <span class="n">period</span> \
        <span class="k">for</span> <span class="n">sub</span><span class="p">,</span> <span class="n">period</span> <span class="ow">in</span> <span class="n">reddit_urls</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># get the subreddit from the URL</span>
        <span class="n">sub</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">4</span><span class="p">]</span>

        <span class="c1"># parse thru each of the posts</span>
        <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.thing&#39;</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">RedditItem</span><span class="p">()</span>

            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">today</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;sub&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a.title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a.title::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.unvoted::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;commentsUrl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a.comments::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>

            <span class="k">yield</span> <span class="n">item</span>
</pre></div>


<h4>Notes</h4>
<ul>
<li>
<p>If I wanted to grab the top 50 posts versus only the top 25, I would have to write some code in the parse method that can follow "next page" links and generate new <code>Request</code> objects. <a href="https://doc.scrapy.org/en/latest/intro/tutorial.html#following-links">More details can be found in the docs</a>.</p>
</li>
<li>
<p>Scrapy will remember webpages it has already seen so that's also one less thing for us to worry about. Just another benefit of using a framework: <em>Scrapy's default options are more comprehensive than anything we can quickly hack together</em></p>
</li>
</ul>
<h3>Pipeline into MongoDB</h3>
<p>Once an item is scraped, it can be processed through an Item Pipeline where we perform tasks such as:</p>
<ul>
<li>cleansing HTML data</li>
<li>validating scraped data (checking that the items contain certain fields)</li>
<li>checking for duplicates (and dropping them)</li>
<li>storing the scraped item in a database</li>
</ul>
<p><a href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html">(from Scrapy docs - Item Pipeline)</a></p>
<p>I don't have any post-processing to perform so let's go ahead and store the data in a MongoDB collection. I modified <a href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html#write-items-to-mongodb">an example I found in the Scrapy docs</a> and used Scrapy's <a href="https://doc.scrapy.org/en/latest/topics/logging.html">built-in logging service</a> to make things bit more professional.</p>
<div class="highlight"><pre><span></span><span class="c1"># pipelines.py</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pymongo</span>

<span class="k">class</span> <span class="nc">MongoPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">collection_name</span> <span class="o">=</span> <span class="s1">&#39;top_reddit_posts&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mongo_uri</span><span class="p">,</span> <span class="n">mongo_db</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span> <span class="o">=</span> <span class="n">mongo_uri</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span> <span class="o">=</span> <span class="n">mongo_db</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_URI&#39;</span><span class="p">),</span>
            <span class="n">mongo_db</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_DATABASE&#39;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">collection_name</span><span class="p">]</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Post added to MongoDB&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>


<hr>
<h2>Configure Settings</h2>
<p>Scrapy has <a href="https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref">a ton of settings</a> including scheduling and hooks into AWS. I want to be nice to Reddit so I will play around with the download delay setting so we don't make too many requests in a short amount of time.</p>
<p>We also want to tell Scrapy about our ItemPipeline so it can store the scraped data in MongoDB.</p>
<div class="highlight"><pre><span></span><span class="c1"># settings.py</span>

<span class="c1"># ...</span>

<span class="c1"># Configure a delay for requests for the same website (default: 0)</span>
<span class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span>
<span class="c1"># See also autothrottle settings and docs</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="o">.</span><span class="mi">25</span>
<span class="n">RANDOMIZE_DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># ...</span>

<span class="c1"># Configure item pipelines</span>
<span class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span>
<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;reddit.pipelines.MongoPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">MONGO_URI</span> <span class="o">=</span> <span class="s1">&#39;mongodb://localhost:27017&#39;</span>
<span class="n">MONGO_DATABASE</span> <span class="o">=</span> <span class="s1">&#39;sivji-sandbox&#39;</span>

<span class="c1"># ...</span>
</pre></div>


<hr>
<h2>Scraping Reddit Top Posts</h2>
<p>Go to the scrapy project folder in the Terminal and enter the following command to run the scraper:</p>
<div class="highlight"><pre><span></span><span class="go">scrapy crawl post</span>
</pre></div>


<p>I get the following output:</p>
<div class="highlight"><pre><span></span><span class="go">2017-02-04 16:17:41 [root] DEBUG: Post added to MongoDB</span>
<span class="go">2017-02-04 16:17:41 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://www.reddit.com/r/Python/top/?sort=top&amp;t=week&gt;</span>
<span class="go">{&#39;commentsUrl&#39;: &#39;https://www.reddit.com/r/Python/comments/5qwre9/is_wxpython_still_a_thing/&#39;,</span>
<span class="go"> &#39;date&#39;: datetime.datetime(2017, 2, 4, 16, 17, 41, 166322),</span>
<span class="go"> &#39;score&#39;: &#39;24&#39;,</span>
<span class="go"> &#39;sub&#39;: &#39;Python&#39;,</span>
<span class="go"> &#39;title&#39;: &#39;Is wxPython still a thing?&#39;,</span>
<span class="go"> &#39;url&#39;: &#39;/r/Python/comments/5qwre9/is_wxpython_still_a_thing/&#39;}</span>
<span class="go">2017-02-04 16:17:41 [scrapy.core.engine] INFO: Closing spider (finished)</span>
<span class="go">2017-02-04 16:17:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span>
<span class="go">{&#39;downloader/request_bytes&#39;: 1451,</span>
<span class="go"> &#39;downloader/request_count&#39;: 5,</span>
<span class="go"> &#39;downloader/request_method_count/GET&#39;: 5,</span>
<span class="go"> &#39;downloader/response_bytes&#39;: 74227,</span>
<span class="go"> &#39;downloader/response_count&#39;: 5,</span>
<span class="go"> &#39;downloader/response_status_count/200&#39;: 4,</span>
<span class="go"> &#39;downloader/response_status_count/301&#39;: 1,</span>
<span class="go"> &#39;finish_reason&#39;: &#39;finished&#39;,</span>
<span class="go"> &#39;finish_time&#39;: datetime.datetime(2017, 2, 4, 21, 17, 41, 171132),</span>
<span class="go"> &#39;item_scraped_count&#39;: 75,</span>
<span class="go"> &#39;log_count/DEBUG&#39;: 156,</span>
<span class="go"> &#39;log_count/INFO&#39;: 7,</span>
<span class="go"> &#39;response_received_count&#39;: 4,</span>
<span class="go"> &#39;scheduler/dequeued&#39;: 4,</span>
<span class="go"> &#39;scheduler/dequeued/memory&#39;: 4,</span>
<span class="go"> &#39;scheduler/enqueued&#39;: 4,</span>
<span class="go"> &#39;scheduler/enqueued/memory&#39;: 4,</span>
<span class="go"> &#39;start_time&#39;: datetime.datetime(2017, 2, 4, 21, 17, 39, 468336)}</span>
<span class="go">2017-02-04 16:17:41 [scrapy.core.engine] INFO: Spider closed (finished)</span>
</pre></div>


<p>We can see the JSON object that was scraped along with some Scrapy stats and messages.</p>
<p>Let's use <a href="https://www.mongodb.com/products/compass">MongoDB Compass</a> to confirm items got loaded into the DB.</p>
<p><img src="..\images\2_reddit_items_in_mongodb_compass.png" alt="Reddit Items in MongoDB Compass" width=750/></p>
<p>Looks good!</p>
<hr>
<h2>Conclusion</h2>
<p>In this tutorial, we utilized the <a href="https://scrapy.org/">Scrapy framework</a> to scrape Top Posts from Reddit into MongoDB. We can easily modify the above template to scrape any website we want. Yes, even <a href="https://github.com/scrapy-plugins/scrapy-splash">Single Page Applications written in JavaScript</a>.</p>
<p>I've been going out of my way to mention relevant documentation sections for each of the Scrapy modules I modified. I did this for two reasons:</p>
<ol>
<li>To show how great the Scrapy docs are (kudos to the contributors)</li>
<li>Emphasize that we should always look to the documentation when we get stuck.</li>
</ol>
<p>I know it's really easy to Google error messages and find relevant Stack Overflow <em>(Praise Be)</em> posts. But we should always start in the documentation to find our solution. Make sure we are using the library in accordance with best practices.</p>
<p>What if the docs for the library you are using are not good or incomplete? Once you learn the package, it might be worth taking the time to write better docs or a blog documenting your journey. Python is great because of the community. Be part of the community. Contribute wherever you can. Doesn't matter if you're just a beginner, your help is always appreciated.</p>
<hr>
<h2>Next Steps</h2>
<p>Future posts in this series will explore how-to:</p>
<ul>
<li><a href="http://scrapy.readthedocs.io/en/latest/topics/practices.html#run-from-script">run Scrapy from a script</a></li>
<li>create and deploy the script on <a href="http://bluemix.net">Bluemix</a></li>
<li>build a REST API data access layer on top of MongoDB</li>
<li>create a front-end interface for this project</li>
</ul>
  </div>
  
<div class="article-tag-list">
<span class="label label-default">Tags</span>
	<a href="/tag/python.html"><i class="fa fa-tag"></i>python</a>&nbsp;
	<a href="/tag/scrapy.html"><i class="fa fa-tag"></i>scrapy</a>&nbsp;
	<a href="/tag/mongodb.html"><i class="fa fa-tag"></i>mongodb</a>&nbsp;
	<a href="/tag/pymongo.html"><i class="fa fa-tag"></i>pymongo</a>&nbsp;
	<a href="/tag/requests.html"><i class="fa fa-tag"></i>requests</a>&nbsp;
	<a href="/tag/web-scraping.html"><i class="fa fa-tag"></i>web-scraping</a>&nbsp;
	<a href="/tag/automation.html"><i class="fa fa-tag"></i>automation</a>&nbsp;
	<a href="/tag/reddit.html"><i class="fa fa-tag"></i>reddit</a>&nbsp;
</div>  <hr />
  <div class="well well-sm">  <!-- Social media sharing buttons -->

    <!-- Google+ -->
    <div class="g-plus" data-action="share" data-annotation="bubble"></div>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <div class="g-plusone" data-size="medium"></div>&nbsp;

    <!-- Facebook -->
    <div class="fb-like" 
        data-href="/drafts/mongodb-pipelines-in-scrapy.html" 
        data-layout="button_count" 
        data-action="like" data-show-faces="true" 
        data-share="true">
    </div>
    &nbsp;
  </div> <!-- /Social media sharing buttons -->
</article>
        </div><!-- /content -->

        <div class="col-md-3 sidebar-nav" id="sidebar">

<div class="row">

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-comment fa-fw fa-lg"></i> Social</h4>
<ul class="list-unstyled social-links">
    <li><a href="http://linkedin.com/in/alysivji/" target="_blank">
	  <i class="fa fa-linkedin-square fa-fw fa-lg" title="LinkedIn"></i>
		LinkedIn
	</a></li>
    <li><a href="http://github.com/alysivji" target="_blank">
	  <i class="fa fa-github-square fa-fw fa-lg" title="GitHub"></i>
		GitHub
	</a></li>
    <li><a href="https://twitter.com/CaiusSivjus" target="_blank">
	  <i class="fa fa-twitter-square fa-fw fa-lg" title="Twitter"></i>
		Twitter
	</a></li>
</ul>
</div>

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-folder fa-fw fa-lg"></i> Categories</h4>
<ul class="list-unstyled category-links">
</ul>
</div>

</div> <!-- /row -->

<h4><i class="fa fa-tags fa-fw fa-lg"></i> Tags</h4>
<p class="tag-cloud">
</p>

<hr />

        </div><!--/sidebar -->
      </div><!--/row-->
    </div><!--/.container /#main-container -->

    <footer id="site-footer">
 
      <address id="site-colophon">
        <p class="text-center text-muted">
        Site built using <a href="http://getpelican.com/" target="_blank">Pelican</a>
        &nbsp;&bull;&nbsp; Theme based on
        <a href="http://www.voidynullness.net/page/voidy-bootstrap-pelican-theme/"
           target="_blank">VoidyBootstrap</a> by 
        <a href="http://www.robertiwancz.com/"
           target="_blank">RKI</a>  
        </p>
      </address><!-- /colophon  -->
    </footer>


    <!-- javascript -->
   
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
            integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
            crossorigin="anonymous"></script>


<!-- Facebook -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script>

<!-- Twitter -->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<!-- Google+ -->
<!-- Synchronous 
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
-->
<!-- Asynchronous -->
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/platform.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>  </body>
</html>