<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="Aly Sivji" />
    <meta name="generator" content="Pelican (VoidyBootstrap theme)" />
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Scraping Websites into MongoDB using Scrapy Pipelines"/>
    <meta property="og:url" content="./mongodb-pipelines-in-scrapy.html"/>
      <meta property="article:section" content="Tutorials"/>
        <meta property="article:tag" content="python"/>
        <meta property="article:tag" content="how-to"/>
        <meta property="article:tag" content="mongodb"/>
        <meta property="article:tag" content="web-scraping"/>
        <meta property="article:tag" content="reddit-scraper"/>
        <meta property="og:image"
              content="https://alysivji.github.io/images/main/siv.jpeg"/>
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Scraping Websites into MongoDB using Scrapy Pipelines">
    <meta name="twitter:description" content="">
    <meta name="twitter:site" content="@CaiusSivjus">
    <meta name="twitter:creator" content="@CaiusSivjus">
    <meta name="twitter:domain" content=".">
        <meta property="twitter:image"
              content="https://alysivji.github.io/images/main/siv.jpeg"/>

    <title>Scraping Websites into MongoDB using Scrapy Pipelines - Siv Scripts</title>

        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
              integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
              crossorigin="anonymous" />
      <link rel="stylesheet"
            href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
            integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN"
            crossorigin="anonymous">


      <link rel="stylesheet" href="./theme/css/tango.css" />
      <link rel="stylesheet" href="./theme/css/voidybootstrap.css" />
      <link rel="stylesheet" href="./theme/css/dataframe.css" />
      <link rel="stylesheet" href="./theme/css/custom.css" />

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js" integrity="sha384-FFgGfda92tXC8nCNOxrCQ3R8x1TNkMFqDZVQdDaaJiiVbjkPBXIJBx0o7ETjy8Bh" crossorigin="anonymous"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js" integrity="sha384-ZoaMbDF+4LeFxg6WdScQ9nnR1QC2MIRxA1O9KWEXQwns1G8UNyIEZIQidzb0T1fo" crossorigin="anonymous"></script>
    <![endif]-->

    <link rel="shortcut icon" href="./files/favicon.ico" />
        <link href="http://alysivji.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Siv Scripts Atom Feed" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90986575-1', '');
  ga('send', 'pageview');

</script>
  </head>

  <body>
    <nav class="navbar navbar-default">
      <div class="container">
	   <div class="navbar-header">
		<button type="button" class="navbar-toggle"
				data-toggle="collapse" data-target="#main-navbar-collapse">
		  <span class="sr-only">Toggle navigation</span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		</button>
		<a class="navbar-brand" href="./" rel="home">
          <i class="fa fa-home fa-fw fa-lg"> </i> </a>
       </div>

      <div class="collapse navbar-collapse" id="main-navbar-collapse">
        <ul class="nav navbar-nav">
              <li>
                <a href="./pages/about.html">About</a>
              </li>
              <li>
                <a href="./pages/talks.html">Talks</a>
              </li>
            <li class="divider"></li>
            <li class="">
              <a href="./archives.html">Archives</a>
            </li>
          <li class="divider"></li>
            <li><a href="http://alysivji.github.io/feeds/all.atom.xml"
                   type="application/atom+xml" rel="alternate">
                <i class="fa fa-rss fa-fw fa-lg"></i> </a></li>
        </ul> <!-- /nav -->
      </div> <!-- /navbar-collapse -->
	  </div> <!-- /container -->
    </nav> <!-- /navbar -->

	<div class="jumbotron" id="overview">
	  <div class="container">
		<h1><a href="./">Siv Scripts</a></h1>
		<p class="lead">Solving Problems Using Code</p>
	  </div>
	</div>

    <div class="container" id="main-container">
      <div class="row">
        <div class="col-md-9" id="content">
<article itemscope="itemscope" itemtype="http://schema.org/BlogPosting">
  <header class="article-header">
<abbr class="article-header-date">
  Mon 20 February 2017
</abbr> <h1>
  <a href="./mongodb-pipelines-in-scrapy.html" rel="bookmark"
     title="Permalink to Scraping Websites into MongoDB using Scrapy Pipelines">
    Scraping Websites into MongoDB using Scrapy Pipelines
  </a>
</h1><div class="article-header-info">
  <p>
      Posted by <a href="./author/aly-sivji.html">Aly Sivji</a>
    in 
    <a href="./category/tutorials.html">
      Tutorials</a>
    &nbsp;&nbsp;
  </p>
</div> <!-- /.article-header-info -->  </header>
  <div class="content-body" itemprop="text articleBody">
	<blockquote>
<h4>Summary</h4>
<ul>
<li>Discuss advantages of using Scrapy framework</li>
<li>Create Reddit spider and scrape top posts from list of subreddits</li>
<li>Implement Scrapy pipeline to send scraped data into MongoDB</li>
</ul>
</blockquote>
<hr>
<p>Wouldn't it be great if every website had a <em>free</em> API we could poll to get the data we wanted?</p>
<p>Sure, we <em>could</em> hack together a solution using <a href="http://docs.python-requests.org/en/master/">Requests</a> and <a href="https://www.crummy.com/software/BeautifulSoup/">Beautiful Soup (bs4)</a>, but if we ever wanted to add features like following next page links or creating data validation pipelines, we would have to do a lot more work.</p>
<p>This is where <a href="https://scrapy.org/">Scrapy</a> shines. Scrapy provides an extendible web scraping framework we can utilize to extract structured data. If the website doesn't have an API, we can build a solution to parse the <strong>data</strong> we need into a <strong>format</strong> we can use.</p>
<p>I recommend <a href="https://doc.scrapy.org/en/master/intro/tutorial.html">the Scrapy tutorial from the documentation</a> as an introduction into the terminology and process flow of the framework. This tutorial assumes some familiarity with Scrapy.</p>
<p>Acknowledgements: I used <a href="https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/">this Real Python post</a> as a guide along with the <a href="https://doc.scrapy.org/en/1.3/">latest version of Scrapy docs (v1.3)</a>.</p>
<hr>
<h2>Project Description</h2>
<p><em>Author's Note: Always read the <a href="http://www.robotstxt.org/robotstxt.html">website's robots.txt file</a> before writing a scraper. Be nice when making requests. Be nice in general. It's a good rule to live by.</em></p>
<p>Reddit provides a platform for communities to have deep discussions on very specific topics. To stay on top of news in my areas of interest, I frequent subreddits such as <a href="http://www.reddit.com/r/peloton">/r/peloton</a>, <a href="http://www.reddit.com/r/datascience">/r/datascience</a>, and <a href="http://www.reddit.com/r/python">/r/python</a>. The Reddit voting system, along with the tendency of people to correct others, ensures that I won't have to spend a lot of time filtering information to get what I need.</p>
<p>If I can design and automate a process to generate weekly digests of my most visited subreddits, I can spend less time on Reddit and more time working on projects like this.</p>
<blockquote>
<h4>Process Outline</h4>
<ul>
<li>Given a list of subreddits, scrape 'Top Posts'</li>
<li>Build pipeline to store item in MongoDB</li>
<li>Automate scraping and set up as CRON job or trigger function on Bluemix</li>
<li>Design website to show scraped data stored in MongoDB<ul>
<li>Research into Angular2 vs Django/Flask</li>
</ul>
</li>
</ul>
</blockquote>
<p>This will be the first in a series of posts that will create a fullstack solution to the workflow described above.</p>
<hr>
<h2>What You Need to Follow Along</h2>
<h3>Development Tools (Stack)</h3>
<ul>
<li><a href="https://www.python.org/downloads/">Python 3</a></li>
<li><a href="https://doc.scrapy.org/en/latest/intro/install.html">Scrapy 1.3</a></li>
<li><a href="https://docs.mongodb.com/manual/installation/">MongoDB</a><ul>
<li>Installed locally or use a service like <a href="https://mlab.com/">MLab</a></li>
<li>I will be using a local instance, but you can follow along with <a href="https://docs.mongodb.com/manual/reference/connection-string/">any Mongo URI</a></li>
</ul>
</li>
<li><a href="http://api.mongodb.com/python/current/installation.html">PyMongo</a></li>
</ul>
<h3>Code</h3>
<ul>
<li><a href="https://github.com/alysivji/reddit-top-posts-scrapy">Github Repo</a> - Tag: <a href="https://github.com/alysivji/reddit-top-posts-scrapy/tree/blog-scrapy-part1">blog-scrapy-part1</a></li>
</ul>
<p>We can checkout the code from the git repository as follows:</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> git checkout tags/blog-scrapy-part1
<span class="go">Note: checking out &#39;tags/blog-scrapy-part1&#39;.</span>
</pre></div>


<p>Or we can use <a href="http://kinolien.github.io/gitzip/">GitZip</a> to download the tagged commit by URL.</p>
<hr>
<h2>Create Scrapy Project</h2>
<p>We will utilize Scrapy's <a href="https://doc.scrapy.org/en/latest/topics/commands.html">command line tools</a> to initialize our project.</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> scrapy startproject reddit
<span class="gp">$</span> <span class="nb">cd</span> reddit
</pre></div>


<p>The directory structure should look as follows:</p>
<div class="highlight"><pre><span></span><span class="go">.</span>
<span class="go">├── scrapy.cfg              # deploy configuration file</span>
<span class="go">└── reddit                  # project&#39;s Python module</span>
<span class="go">    ├── __init__.py</span>
<span class="go">    ├── items.py            # items definition file</span>
<span class="go">    ├── middlewares.py</span>
<span class="go">    ├── pipelines.py        # project pipelines file</span>
<span class="go">    ├── settings.py         # project settings</span>
<span class="go">    └── spiders             # dir to store spiders</span>
<span class="go">        └── __init__.py</span>
</pre></div>


<p><a href="https://doc.scrapy.org/en/latest/intro/tutorial.html#creating-a-project">File descriptions from Scrapy docs</a></p>
<hr>
<h2>Implement Crawler</h2>
<h3>Items to Scrape</h3>
<p>Let's take a look at the <a href="https://www.reddit.com/r/peloton/top/?sort=top&amp;t=all">all-time list of top posts from /r/peloton</a>. Below is a screenshot with the items we want to scrape highlighted.</p>
<p><img src="..\images\01-09\1_reddit_items_to_parse.png" alt="Reddit Top Posts -- items to scrape" width=750/></p>
<p>From this we can create our item template. We include some additional pieces of information (subreddit, date) to provide context.</p>
<div class="highlight"><pre><span></span><span class="c1"># items.py</span>

<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">RedditItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Defining the storage containers for the data we</span>
<span class="sd">    plan to scrape</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">date_str</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">sub</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">commentsUrl</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>


<h3>Create Spider</h3>
<p>We will follow the <a href="https://doc.scrapy.org/en/latest/intro/tutorial.html">template from the Scrapy docs</a> to create the spider.</p>
<div class="highlight"><pre><span></span><span class="c1"># spiders/__init__.py</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span> <span class="k">as</span> <span class="n">dt</span>
<span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">reddit.items</span> <span class="kn">import</span> <span class="n">RedditItem</span>

<span class="k">class</span> <span class="nc">PostSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;post&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reddit.com&#39;</span><span class="p">]</span>

    <span class="n">reddit_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;datascience&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;python&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;programming&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;machinelearning&#39;</span><span class="p">,</span> <span class="s1">&#39;week&#39;</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.reddit.com/r/&#39;</span> <span class="o">+</span> <span class="n">sub</span> <span class="o">+</span> <span class="s1">&#39;/top/?sort=top&amp;t=&#39;</span> <span class="o">+</span> <span class="n">period</span> \
        <span class="k">for</span> <span class="n">sub</span><span class="p">,</span> <span class="n">period</span> <span class="ow">in</span> <span class="n">reddit_urls</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># get the subreddit from the URL</span>
        <span class="n">sub</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">4</span><span class="p">]</span>

        <span class="c1"># parse thru each of the posts</span>
        <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.thing&#39;</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">RedditItem</span><span class="p">()</span>

            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">today</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;date_str&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;sub&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a.title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>

            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a.title::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
            <span class="c1">## if self-post, add reddit base url (as it&#39;s relative by default)</span>
            <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">][:</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;/r/&#39;</span><span class="p">:</span>
                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;https://www.reddit.com&#39;</span> <span class="o">+</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">]</span>

            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.unvoted::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">())</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;commentsUrl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a.comments::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>

            <span class="k">yield</span> <span class="n">item</span>
</pre></div>


<p>The first part of the code defines the spider settings and tells Scrapy which URLs to parse (<code>start_urls</code> variable). <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions">Shoutout to list comprehensions</a>!</p>
<p>The parse function defines how Scrapy will process each of the downloaded reponses (<a href="https://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse">docs</a>). We use CSS selectors to extract data from the HTML (<a href="https://doc.scrapy.org/en/latest/topics/selectors.html">more details in the Scrapy docs</a>) before we <code>yield</code> items back to the framework using <a href="https://wiki.python.org/moin/Generators">generators</a>.</p>
<h5>Notes</h5>
<ul>
<li>
<p>Generators are awesome because generators are Pythonic. If possible, try to <code>yield</code> vs <code>return</code>. <a href="http://dabeaz.com/">David Beazley</a> covered generators in <a href="https://www.youtube.com/watch?v=5-qadlG7tWo">his tutorial from PyCon 2014</a>. <strong>Must watch for Pythonistas</strong>.</p>
</li>
<li>
<p>If we want to grab the top 50 posts versus the top 25, we would need to modify the parse method to follow "next page" links and generate new <code>Request</code> objects. <a href="https://doc.scrapy.org/en/latest/intro/tutorial.html#following-links">More details can be found in the docs</a>.</p>
</li>
<li>
<p>Scrapy keeps track of visited webpages to prevent scraping the same URL more than once. This is yet another benefit of using a framework: <em>Scrapy's default options are more comprehensive than anything we can quickly hack together</em>.</p>
</li>
</ul>
<h3>Pipeline into MongoDB</h3>
<p>Once an item is scraped, it can be processed through an Item Pipeline where we perform tasks such as:</p>
<ul>
<li>cleansing HTML data</li>
<li>validating scraped data (checking that the items contain certain fields)</li>
<li>checking for duplicates (and dropping them)</li>
<li>storing the scraped item in a database</li>
</ul>
<p><a href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html">(from Scrapy docs - Item Pipeline)</a></p>
<p>We don't have any post-processing to perform so let's go ahead and store the data in a MongoDB collection. We will modify <a href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html#write-items-to-mongodb">an example I found in the Scrapy docs</a> and use Scrapy's <a href="https://doc.scrapy.org/en/latest/topics/logging.html">built-in logging service</a> to make things a bit more professional.</p>
<div class="highlight"><pre><span></span><span class="c1"># pipelines.py</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pymongo</span>

<span class="k">class</span> <span class="nc">MongoPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">collection_name</span> <span class="o">=</span> <span class="s1">&#39;top_reddit_posts&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mongo_uri</span><span class="p">,</span> <span class="n">mongo_db</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span> <span class="o">=</span> <span class="n">mongo_uri</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span> <span class="o">=</span> <span class="n">mongo_db</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="c1">## pull in information from settings.py</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_URI&#39;</span><span class="p">),</span>
            <span class="n">mongo_db</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_DATABASE&#39;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="c1">## initializing spider</span>
        <span class="c1">## opening db connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="c1">## clean up when spider is closed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="c1">## how to handle each post</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">collection_name</span><span class="p">]</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Post added to MongoDB&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>


<hr>
<h2>Configure Settings</h2>
<p>Scrapy has <a href="https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref">a lot of settings</a> for us to configure. We will be nice to Reddit and add a randomized download delay; this will ensure that we don't make too many requests in a short amount of time.</p>
<p>We also want to tell Scrapy about our MongoDB and ItemPipeline so it can import modules as necessary.</p>
<div class="highlight"><pre><span></span><span class="c1"># settings.py</span>

<span class="c1"># ...</span>

<span class="c1"># Configure a delay for requests for the same website (default: 0)</span>
<span class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span>
<span class="c1"># See also autothrottle settings and docs</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="o">.</span><span class="mi">25</span>
<span class="n">RANDOMIZE_DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># ...</span>

<span class="c1"># Configure item pipelines</span>
<span class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span>
<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;reddit.pipelines.MongoPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">MONGO_URI</span> <span class="o">=</span> <span class="s1">&#39;mongodb://localhost:27017&#39;</span>
<span class="n">MONGO_DATABASE</span> <span class="o">=</span> <span class="s1">&#39;sivji-sandbox&#39;</span>

<span class="c1"># ...</span>
</pre></div>


<hr>
<h2>Scraping Reddit Top Posts</h2>
<p>We can now run the scraper using the following command from the scrapy project folder:</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> scrapy crawl post
<span class="go">2017-02-04 16:17:41 [root] DEBUG: Post added to MongoDB</span>
<span class="go">2017-02-04 16:17:41 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://www.reddit.com/r/Python/top/?sort=top&amp;t=week&gt;</span>
<span class="go">{&#39;commentsUrl&#39;: &#39;https://www.reddit.com/r/Python/comments/5qwre9/is_wxpython_still_a_thing/&#39;,</span>
<span class="go"> &#39;date&#39;: datetime.datetime(2017, 2, 4, 16, 17, 41, 166322),</span>
<span class="go"> &#39;score&#39;: &#39;24&#39;,</span>
<span class="go"> &#39;sub&#39;: &#39;Python&#39;,</span>
<span class="go"> &#39;title&#39;: &#39;Is wxPython still a thing?&#39;,</span>
<span class="go"> &#39;url&#39;: &#39;/r/Python/comments/5qwre9/is_wxpython_still_a_thing/&#39;}</span>
<span class="go">2017-02-04 16:17:41 [scrapy.core.engine] INFO: Closing spider (finished)</span>
<span class="go">2017-02-04 16:17:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span>
<span class="go">{&#39;downloader/request_bytes&#39;: 1451,</span>
<span class="go"> &#39;downloader/request_count&#39;: 5,</span>
<span class="go"> &#39;downloader/request_method_count/GET&#39;: 5,</span>
<span class="go"> &#39;downloader/response_bytes&#39;: 74227,</span>
<span class="go"> &#39;downloader/response_count&#39;: 5,</span>
<span class="go"> &#39;downloader/response_status_count/200&#39;: 4,</span>
<span class="go"> &#39;downloader/response_status_count/301&#39;: 1,</span>
<span class="go"> &#39;finish_reason&#39;: &#39;finished&#39;,</span>
<span class="go"> &#39;finish_time&#39;: datetime.datetime(2017, 2, 4, 21, 17, 41, 171132),</span>
<span class="go"> &#39;item_scraped_count&#39;: 75,</span>
<span class="go"> &#39;log_count/DEBUG&#39;: 156,</span>
<span class="go"> &#39;log_count/INFO&#39;: 7,</span>
<span class="go"> &#39;response_received_count&#39;: 4,</span>
<span class="go"> &#39;scheduler/dequeued&#39;: 4,</span>
<span class="go"> &#39;scheduler/dequeued/memory&#39;: 4,</span>
<span class="go"> &#39;scheduler/enqueued&#39;: 4,</span>
<span class="go"> &#39;scheduler/enqueued/memory&#39;: 4,</span>
<span class="go"> &#39;start_time&#39;: datetime.datetime(2017, 2, 4, 21, 17, 39, 468336)}</span>
<span class="go">2017-02-04 16:17:41 [scrapy.core.engine] INFO: Spider closed (finished)</span>
</pre></div>


<p>We see the JSON objects that were scraped.</p>
<p>We can confirm the data was stored in the database using <a href="https://www.mongodb.com/products/compass">MongoDB Compass</a>.</p>
<p><img src="..\images\01-09\1_reddit_items_in_mongodb_compass.png" alt="Reddit Items in MongoDB Compass" width=750/></p>
<p>Looks good!</p>
<hr>
<h2>Conclusion</h2>
<p>In this tutorial, we utilized the <a href="https://scrapy.org/">Scrapy framework</a> to scrape Top Posts from Reddit into MongoDB. The above template can be modified to scrape any website we want. Yes, even <a href="https://github.com/scrapy-plugins/scrapy-splash">Single Page Applications written in JavaScript</a>!</p>
<p>I've been going out of my way to mention relevant documentation sections for each of the Scrapy modules I utilized. I did this for two reasons:</p>
<ol>
<li>To show how great the Scrapy docs are (kudos to the contributors)</li>
<li>To emphasize that we should always look to the documentation when we get stuck.</li>
</ol>
<p>It's easy to Google error messages and find relevant Stack Overflow <em>(Praise Be)</em> posts. However, we should always start our search for a solution in the documentation. Additionally, we can gain insights into best practices as described by the authors of the package.</p>
<p>What if the documentation we are using is awful or incomplete? Once we are done with our project, we should take the time to write better docs or a blog entry documenting our experience. Make it easier for the next person.</p>
<p>Python is great because of the community. Become part of the community and contribute wherever you can. It doesn't matter if you're a beginner, your help is always appreciated.</p>
<hr>
<h2>Next Steps</h2>
<p>Future posts in this series will explore:</p>
<ul>
<li>sending an email digest of scraped data</li>
<li><a href="http://scrapy.readthedocs.io/en/latest/topics/practices.html#run-from-script">running Scrapy from a script</a><ul>
<li>deploying and scheduling the script to run on <a href="https://www.pythonanywhere.com">Python Anywhere</a></li>
</ul>
</li>
<li>building a REST API data access layer on top of MongoDB</li>
<li>creating a front-end interface for users<ul>
<li>deploying front-end on <a href="https://www.pythonanywhere.com">Python Anywhere</a></li>
<li>deploying front-end as a Cloud Foundary app on <a href="http://bluemix.net">Bluemix</a></li>
</ul>
</li>
</ul>
<hr>
<h2>Updates</h2>
<ul>
<li><strong>02/24</strong>: Added a field (date_str) to the top_reddit_posts collection to make indices more effective.</li>
<li><strong>03/06</strong>: Fixed spelling and formatting</li>
<li><strong>03/22</strong>: Added instructions on pulling tagged commit from Git repo</li>
</ul>
  </div>

<div class="article-tag-list">
<span class="label label-default">Tags</span>
	<a href="./tag/python.html"><i class="fa fa-tag"></i>python</a>&nbsp;
	<a href="./tag/how-to.html"><i class="fa fa-tag"></i>how-to</a>&nbsp;
	<a href="./tag/mongodb.html"><i class="fa fa-tag"></i>mongodb</a>&nbsp;
	<a href="./tag/web-scraping.html"><i class="fa fa-tag"></i>web-scraping</a>&nbsp;
	<a href="./tag/reddit-scraper.html"><i class="fa fa-tag"></i>reddit-scraper</a>&nbsp;
</div>  <hr />
  <div class="well well-sm">  <!-- Social media sharing buttons -->
    <!-- Twitter -->
    <a href="https://twitter.com/share" class="twitter-share-button" 
       data-via="CaiusSivjus" >Tweet</a>&nbsp;

    <!-- Google+ -->
    <div class="g-plus" data-action="share" data-annotation="bubble"></div>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <div class="g-plusone" data-size="medium"></div>&nbsp;

    <!-- Facebook -->
    <div class="fb-like" 
        data-href="./mongodb-pipelines-in-scrapy.html" 
        data-layout="button_count" 
        data-action="like" data-show-faces="true" 
        data-share="true">
    </div>
    &nbsp;
  </div> <!-- /Social media sharing buttons -->
  <div class="comments">
	<h2>Comments</h2>
	<div id="disqus_thread"></div>
	<script type="text/javascript">
				   (function() {
						var dsq = document.createElement('script');
						dsq.type = 'text/javascript'; dsq.async = true;
						dsq.src = '//siv-scripts.disqus.com/embed.js';
						(document.getElementsByTagName('head')[0] ||
						 document.getElementsByTagName('body')[0]).appendChild(dsq);
				  })();
	</script>
  </div>
</article>
        </div><!-- /content -->

        <div class="col-md-3 sidebar-nav" id="sidebar">

<div class="row">

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-comment fa-fw fa-lg"></i> Social</h4>
<ul class="list-unstyled social-links">
    <li><a href="http://linkedin.com/in/alysivji/" target="_blank">
	  <i class="fa fa-linkedin-square fa-fw fa-lg" title="LinkedIn"></i>
		LinkedIn
	</a></li>
    <li><a href="http://github.com/alysivji" target="_blank">
	  <i class="fa fa-github-square fa-fw fa-lg" title="GitHub"></i>
		GitHub
	</a></li>
    <li><a href="https://twitter.com/CaiusSivjus" target="_blank">
	  <i class="fa fa-twitter-square fa-fw fa-lg" title="Twitter"></i>
		Twitter
	</a></li>
</ul>
</div>

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-folder fa-fw fa-lg"></i> Categories</h4>
<ul class="list-unstyled category-links">
  <li><a href="./category/data-analysis.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Data Analysis</a></li>
  <li><a href="./category/deep-dives.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Deep Dives</a></li>
  <li><a href="./category/quick-hits.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Quick Hits</a></li>
  <li><a href="./category/thoughts.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Thoughts</a></li>
  <li><a href="./category/tutorials.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Tutorials</a></li>
</ul>
</div>

</div> <!-- /row -->

<h4><i class="fa fa-tags fa-fw fa-lg"></i> Tags</h4>
<p class="tag-cloud">
    <span class="tag-3">
      <a href="./tag/grad-school.html">
          <i class="fa fa-tag"></i>
        grad-school
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/software-engineering.html">
          <i class="fa fa-tag"></i>
        software-engineering
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/math.html">
          <i class="fa fa-tag"></i>
        math
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/flask.html">
          <i class="fa fa-tag"></i>
        flask
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/terminal.html">
          <i class="fa fa-tag"></i>
        terminal
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/art-of-developer-testing.html">
          <i class="fa fa-tag"></i>
        art-of-developer-testing
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/productivity.html">
          <i class="fa fa-tag"></i>
        productivity
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/vim.html">
          <i class="fa fa-tag"></i>
        vim
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/web-scraping.html">
          <i class="fa fa-tag"></i>
        web-scraping
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/docker.html">
          <i class="fa fa-tag"></i>
        docker
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/review.html">
          <i class="fa fa-tag"></i>
        review
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/kubernetes.html">
          <i class="fa fa-tag"></i>
        kubernetes
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/gis.html">
          <i class="fa fa-tag"></i>
        gis
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/elasticsearch.html">
          <i class="fa fa-tag"></i>
        elasticsearch
      </a>
    </span>
    <span class="tag-1">
      <a href="./tag/how-to.html">
          <i class="fa fa-tag"></i>
        how-to
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/workflow.html">
          <i class="fa fa-tag"></i>
        workflow
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/web-development.html">
          <i class="fa fa-tag"></i>
        web-development
      </a>
    </span>
    <span class="tag-1">
      <a href="./tag/python.html">
          <i class="fa fa-tag"></i>
        python
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/reddit-scraper.html">
          <i class="fa fa-tag"></i>
        reddit-scraper
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/data-viz.html">
          <i class="fa fa-tag"></i>
        data-viz
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/mac.html">
          <i class="fa fa-tag"></i>
        mac
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/cest-la-vie.html">
          <i class="fa fa-tag"></i>
        c'est-la-vie
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/exploring-pypi.html">
          <i class="fa fa-tag"></i>
        exploring-pypi
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/php.html">
          <i class="fa fa-tag"></i>
        php
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/pandas.html">
          <i class="fa fa-tag"></i>
        pandas
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/testing.html">
          <i class="fa fa-tag"></i>
        testing
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/community.html">
          <i class="fa fa-tag"></i>
        community
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/data-science.html">
          <i class="fa fa-tag"></i>
        data-science
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/pythonic.html">
          <i class="fa fa-tag"></i>
        pythonic
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/book.html">
          <i class="fa fa-tag"></i>
        book
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/mongodb.html">
          <i class="fa fa-tag"></i>
        mongodb
      </a>
    </span>
</p>
<h4><i class="fa fa-rss fa-fw fa-lg"></i> Feeds</h4>
<ul class="list-unstyled">
    <li><a href="http://alysivji.github.io/feeds/all.atom.xml" 
		   type="application/atom+xml" rel="alternate">
		<i class="fa fa-rss-square fa-fw fa-lg"></i> Atom Feed</a></li>
</ul>

<hr />

        </div><!--/sidebar -->
      </div><!--/row-->
    </div><!--/.container /#main-container -->

    <footer id="site-footer">
      <address id="site-colophon">
        <p class="text-center text-muted">
        Site built using <a href="http://getpelican.com/" target="_blank">Pelican</a>
        &nbsp;&bull;&nbsp; Theme based on
        <a href="http://www.voidynullness.net/page/voidy-bootstrap-pelican-theme/"
           target="_blank">VoidyBootstrap</a> by
        <a href="http://www.robertiwancz.com/"
           target="_blank">RKI</a>
        </p>
      </address><!-- /colophon  -->
    </footer>

<!-- DISQUS script for displaying comment count -->
<script type="text/javascript">
    var disqus_shortname = 'siv-scripts';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>

    <!-- javascript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
            integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
            crossorigin="anonymous"></script>


<!-- Facebook -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script>

<!-- Twitter -->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<!-- Google+ -->
<!-- Synchronous 
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
-->
<!-- Asynchronous -->
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/platform.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>  </body>
</html>