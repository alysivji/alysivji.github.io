<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="Aly Sivji" />
    <meta name="generator" content="Pelican (VoidyBootstrap theme)" />
    <meta name="twitter:card" content="summary"> 
    <meta name="twitter:title" content="Scheduling Web Scrapers on the PythonAnywhere Cloud (Scrapy Part 2)">
    <meta name="twitter:description" content="">
    <meta name="twitter:site" content="@CaiusSivjus">
    <meta name="twitter:creator" content="@CaiusSivjus">
    <meta name="twitter:domain" content=".">

    <title>Scheduling Web Scrapers on the PythonAnywhere Cloud (Scrapy Part 2) - Siv Scripts</title>

   
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
              integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
              crossorigin="anonymous" />
      <link rel="stylesheet"
            href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
            integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN"
            crossorigin="anonymous">


      <link rel="stylesheet" href="./theme/css/tango.css" />
      <link rel="stylesheet" href="./theme/css/voidybootstrap.css" />
      <link rel="stylesheet" href="./theme/css/dataframe.css" />
      <link rel="stylesheet" href="./theme/css/custom.css" />

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js" integrity="sha384-FFgGfda92tXC8nCNOxrCQ3R8x1TNkMFqDZVQdDaaJiiVbjkPBXIJBx0o7ETjy8Bh" crossorigin="anonymous"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js" integrity="sha384-ZoaMbDF+4LeFxg6WdScQ9nnR1QC2MIRxA1O9KWEXQwns1G8UNyIEZIQidzb0T1fo" crossorigin="anonymous"></script>
    <![endif]-->

    <link rel="shortcut icon" href="./favicon.ico" />
        <link href="http://alysivji.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Siv Scripts Atom Feed" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90986575-1', '');
  ga('send', 'pageview');

</script>
  </head>

  <body>
   
    <nav class="navbar navbar-default">
      <div class="container">
	   <div class="navbar-header">
		<button type="button" class="navbar-toggle" 
				data-toggle="collapse" data-target="#main-navbar-collapse">
		  <span class="sr-only">Toggle navigation</span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		</button>
		<a class="navbar-brand" href="./" rel="home">
          <i class="fa fa-home fa-fw fa-lg"> </i> </a>
       </div>

      <div class="collapse navbar-collapse" id="main-navbar-collapse">
        <ul class="nav navbar-nav">
              <li>
                <a href="./pages/about.html">About</a>
              </li>
              <li>
                <a href="./pages/talks.html">Talks</a>
              </li>
            <li class="divider"></li>
            <li class="">
              <a href="./archives.html">Archives</a>
            </li>
          <li class="divider"></li>
            <li><a href="http://alysivji.github.io/feeds/all.atom.xml" 
                   type="application/atom+xml" rel="alternate">
                <i class="fa fa-rss fa-fw fa-lg"></i> </a></li>
        </ul> <!-- /nav -->
      </div> <!-- /navbar-collapse -->
	  </div> <!-- /container -->
    </nav> <!-- /navbar -->

	<div class="jumbotron" id="overview">
	  <div class="container">
		<h1><a href="./">Siv Scripts</a></h1>
		<p class="lead">Solving Problems Using Code</p>
	  </div>
	</div>

    <div class="container" id="main-container">
      <div class="row">
        <div class="col-md-9" id="content">
<article itemscope="itemscope" itemtype="http://schema.org/BlogPosting">
  <header class="article-header">
<abbr class="article-header-date">
  Wed 12 April 2017
</abbr> <h1>
  <a href="./scrapy-part2-running-scrapy-on-pythonanywhere.html" rel="bookmark"
     title="Permalink to Scheduling Web Scrapers on the PythonAnywhere Cloud (Scrapy Part 2)">
    Scheduling Web Scrapers on the PythonAnywhere Cloud (Scrapy Part 2)
  </a>
</h1><div class="article-header-info">
  <p>
      Posted by <a href="./author/aly-sivji.html">Aly Sivji</a>
    in 
    <a href="./category/tutorials.html">
      Tutorials</a>
    &nbsp;&nbsp;
  </p>
</div> <!-- /.article-header-info -->  </header>
  <div class="content-body" itemprop="text articleBody">
	<p>(Note: This post is part of my <a href="https://alysivji.github.io/tag/reddit-scraper.html">reddit-scraper series</a>)</p>
<blockquote>
<h4>Summary</h4>
<ul>
<li>Running Scrapy spider as a script</li>
<li>Scheduling script to run on PythonAnywhere cloud</li>
</ul>
</blockquote>
<hr>
<p><a href="https://alysivji.github.io/mongodb-pipelines-in-scrapy.html">Previously on Siv Scripts</a>, we created a web scraping pipeline to pull Top Posts from Reddit and store them in a MongoDB collection. At this stage, we still have to manually execute our crawler via the command-line interface (CLI) each time we want to scrape Reddit.</p>
<p><em>Cue <a href="https://twitter.com/raymondh">Raymond Hettinger</a>:</em> There <strong>MUST</strong> be a better way!</p>
<p>And there is!</p>
<p>In this post, we will convert our command-line Scrapy application into a script that we will schedule to run on the <a href="https://www.pythonanywhere.com">Python Anywhere</a> cloud platform. In addition, we will use our <code>top_post_emailer</code> module to <a href="https://alysivji.github.io/flask-part1-generating-html-pages-with-mongoengine-jinja2.html">automatically send ourselves an email digest</a> of the data that was scraped.</p>
<hr>
<h2>What You Need to Follow Along</h2>
<h3>Development Tools (Stack)</h3>
<ul>
<li><a href="https://www.python.org/downloads/">Python 3.5</a></li>
<li><a href="https://www.pythonanywhere.com">PythonAnywhere account</a></li>
<li><a href="https://docs.mongodb.com/manual/installation/">MongoDB</a><ul>
<li>Cloud accessible installation -- <a href="https://mlab.com/">MLab</a></li>
<li>Free Sandbox plan provides single database with 0.5 GB storage on a shared VM</li>
</ul>
</li>
<li><a href="https://mailgun.com/signup">MailGun account</a><ul>
<li>Free account is limited to 10K emails per month, more than enough for our purposes</li>
</ul>
</li>
</ul>
<h3>Code</h3>
<ul>
<li><a href="https://github.com/alysivji/reddit-top-posts-scrapy">Github Repo</a> - Tag: <a href="https://github.com/alysivji/reddit-top-posts-scrapy/tree/blog-scrapy-part2">blog-scrapy-part2</a></li>
</ul>
<p>We can checkout the code from the git repository as follows:</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> git checkout tags/blog-scrapy-part2
<span class="go">Note: checking out &#39;tags/blog-scrapy-part2&#39;.</span>
</pre></div>


<p>Or we can use <a href="http://kinolien.github.io/gitzip/">GitZip</a> to download the tagged commit by URL.</p>
<hr>
<h2>Setting up MongoDB on mLab</h2>
<p>Up until now we have been using a local instance of MongoDB to store our scraped data. Since we will be running our scraper on the cloud, we will also need to create an online instance of our database where we can store data.</p>
<p>Head over to <a href="https://mlab.com/">mLab</a> to create an account and set up a collection in our sandbox database to store the data.</p>
<p>Let's initialize our MongoDB collection with documents from our local collection. <a href="https://docs.mongodb.com/manual/core/backups/">From the MongoDB docs</a>:</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> mongoexport --db sivji-sandbox --collection top_reddit_posts --out <span class="m">20170322</span>-reddit-posts.json
<span class="go">2017-03-22T23:18:39.404-0500    connected to: localhost</span>
<span class="go">2017-03-22T23:18:39.429-0500    exported 751 records</span>

<span class="gp">$</span> mongoimport -h &lt;mlab-url&gt;:port -d sivji-sandbox -c top_reddit_posts -u &lt;dbuser&gt; -p &lt;dbpass&gt; --file <span class="m">20170322</span>-reddit-posts.json
<span class="go">2017-03-22T23:21:14.075-0500    connected to: &lt;mlab-url&gt;:port</span>
<span class="go">2017-03-22T23:21:15.504-0500    imported 751 documents</span>
</pre></div>


<p>We can use <a href="https://www.mongodb.com/products/compass">MongoDB Compass</a> to view our mLab instance and create indexes for our collection. I covered how to do this in a <a href="https://alysivji.github.io/flask-part1-generating-html-pages-with-mongoengine-jinja2.html#creating-mongodb-indexes">previous post</a>.</p>
<hr>
<h2>Running Scrapy From a Script</h2>
<p>Currently we run our spider using the <code>scrapy crawl</code> command via Scrapy's CLI. Looking through the Scrapy documentation, we see that we can <a href="http://scrapy.readthedocs.io/en/latest/topics/practices.html#run-from-script">utilize Scrapy's API and run our scraper as a script</a>.</p>
<p>In our Scrapy directory, let's add the following file:</p>
<div class="highlight"><pre><span></span><span class="c1"># app.py (v1)</span>

<span class="sd">&quot;&quot;&quot;Script to crawl Top Posts across sub reddits and store results in MongoDB</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>
<span class="kn">from</span> <span class="nn">reddit.spiders</span> <span class="kn">import</span> <span class="n">PostSpider</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">get_project_settings</span><span class="p">())</span>

    <span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">PostSpider</span><span class="p">)</span>
    <span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>


<p>Let's also take this opportunity to modify our Scrapy project to use our mLab MongoDB instance. This will require us to change the following files:</p>
<div class="highlight"><pre><span></span><span class="c1"># settings.py</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">configparser</span>

<span class="c1"># ...</span>

<span class="c1"># Configure item pipelines</span>
<span class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span>
<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;reddit.pipelines.MongoPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1">## get mongodb params (using configparser)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">configparser</span><span class="o">.</span><span class="n">ConfigParser</span><span class="p">()</span>
<span class="n">config</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)),</span> <span class="s1">&#39;settings.cfg&#39;</span><span class="p">))</span>
<span class="n">mlab_uri</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MongoDB&#39;</span><span class="p">,</span> <span class="s1">&#39;mlab_uri&#39;</span><span class="p">)</span>

<span class="n">MONGO_URI</span> <span class="o">=</span> <span class="n">mlab_uri</span>
<span class="n">MONGO_DATABASE</span> <span class="o">=</span> <span class="s1">&#39;sivji-sandbox&#39;</span>

<span class="c1"># ...</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># top_post_emailer/settings.cfg</span>

<span class="p">[</span><span class="n">MongoDB</span><span class="p">]</span>
<span class="n">mlab_uri</span> <span class="o">=</span> <span class="p">[</span><span class="n">Your</span> <span class="n">mLab</span> <span class="n">URI</span> <span class="n">here</span><span class="p">]</span>
</pre></div>


<p>Our Scrapy project folder should have the following structure:</p>
<div class="highlight"><pre><span></span><span class="go">.</span>
<span class="go">├── app.py</span>
<span class="go">├── reddit</span>
<span class="go">│   ├── __init__.py</span>
<span class="go">│   ├── items.py</span>
<span class="go">│   ├── middlewares.py</span>
<span class="go">│   ├── pipelines.py</span>
<span class="go">│   ├── settings.cfg</span>
<span class="go">│   ├── settings.py</span>
<span class="go">│   └── spiders</span>
<span class="go">│       └── __init__.py</span>
<span class="go">└── scrapy.cfg</span>
</pre></div>


<p>We can execute our Reddit Top Posts scraper as a script by running  <code>app.py</code> in a Python interpreter.</p>
<h4>Aside: Testing Web Scrapers</h4>
<p>How should we test web scrapers?</p>
<p>We could download an offline copy of the webpage we are scraping and use that to test changes to our <code>parse()</code> method. Let's think about this a bit more: what happens if the website changes its layout and our offline copy becomes stale? A StackOverflow <em>(Praise Be)</em> user <a href="http://stackoverflow.com/a/12741030/4326704">suggested a good workflow for this situation</a>.</p>
<p>We could also consider utilizing <a href="https://doc.scrapy.org/en/latest/topics/contracts.html">Spider contracts</a> to check if we are getting expected results for a specified URL. Be aware of the <a href="http://stackoverflow.com/a/38193550/4326704">limitations and issues with contracts</a>.</p>
<p>We will not be implementing any kind of unit tests for this scraper. The Reddit layout is more-or-less static. If the website changes, we will notice this in the email digests we receive and we can make the necessary modifications at that time. For projects that are a bit more critical than our toy scraper, we should probably set up unit tests as described above.</p>
<hr>
<h2><code>import</code> Email Functionality</h2>
<p>In a previous post, we <a href="https://alysivji.github.io/flask-part1-generating-html-pages-with-mongoengine-jinja2.html">created the <code>top_post_emailer</code> to send ourselves email digests</a>. Let's <code>import</code> this package and wire it into our script.</p>
<p>If you followed the previous post, copy the <code>top_post_emailer</code> folder into the Scrapy project folder. If you do not already have this package, you can download <a href="https://github.com/alysivji/reddit-top-posts-email-from-mongodb/tree/master/top_post_emailer">this folder from Github</a> using <a href="http://kinolien.github.io/gitzip/">GitZip</a> and extract it into the Scrapy project folder.</p>
<p>Since we are using mLab as our database, we will need to update the following files:</p>
<div class="highlight"><pre><span></span><span class="c1"># top_post_emailer/__init__.py</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">configparser</span>
<span class="kn">from</span> <span class="nn">mongoengine.connection</span> <span class="kn">import</span> <span class="n">connect</span>
<span class="kn">from</span> <span class="nn">.data_model</span> <span class="kn">import</span> <span class="n">Post</span>
<span class="kn">from</span> <span class="nn">.render_template</span> <span class="kn">import</span> <span class="n">render</span>
<span class="kn">from</span> <span class="nn">.mailgun_emailer</span> <span class="kn">import</span> <span class="n">send_email</span>

<span class="k">def</span> <span class="nf">email_last_scraped_date</span><span class="p">():</span>
    <span class="c1">## mongodb params (using configparser)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">configparser</span><span class="o">.</span><span class="n">ConfigParser</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)),</span> <span class="s1">&#39;settings.cfg&#39;</span><span class="p">))</span>
    <span class="n">mlab_uri</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MongoDB&#39;</span><span class="p">,</span> <span class="s1">&#39;mlab_uri&#39;</span><span class="p">)</span>

    <span class="c1"># connect to db</span>
    <span class="n">MONGO_URI</span> <span class="o">=</span> <span class="n">mlab_uri</span>
    <span class="n">connect</span><span class="p">(</span><span class="s1">&#39;sivji-sandbox&#39;</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="n">MONGO_URI</span><span class="p">)</span>    

    <span class="c1">## get the last date the webscraper was run</span>
    <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">Post</span><span class="o">.</span><span class="n">objects</span><span class="p">()</span><span class="o">.</span><span class="n">fields</span><span class="p">(</span><span class="n">date_str</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">order_by</span><span class="p">(</span><span class="s1">&#39;-date_str&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">day_to_pull</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">date_str</span>

    <span class="c1">## pass in variables, render template, and send</span>
    <span class="n">context</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;day_to_pull&#39;</span><span class="p">:</span> <span class="n">day_to_pull</span><span class="p">,</span>
        <span class="s1">&#39;Post&#39;</span><span class="p">:</span> <span class="n">Post</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">html</span> <span class="o">=</span> <span class="n">render</span><span class="p">(</span><span class="s2">&quot;template.html&quot;</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
    <span class="n">send_email</span><span class="p">(</span><span class="n">html</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># top_post_emailer/settings.cfg</span>

<span class="p">[</span><span class="n">MailGun</span><span class="p">]</span>
<span class="n">api</span> <span class="o">=</span> <span class="p">[</span><span class="n">Your</span> <span class="n">MailGun</span> <span class="n">API</span> <span class="n">key</span> <span class="n">here</span><span class="p">]</span>
<span class="n">domain</span> <span class="o">=</span> <span class="p">[</span><span class="n">Your</span> <span class="n">MailGun</span> <span class="n">domain</span> <span class="n">here</span><span class="p">]</span>

<span class="p">[</span><span class="n">MongoDB</span><span class="p">]</span>
<span class="n">mlab_uri</span> <span class="o">=</span> <span class="p">[</span><span class="n">Your</span> <span class="n">mLab</span> <span class="n">URI</span> <span class="n">here</span><span class="p">]</span>
</pre></div>


<p>Let's add email functionality to our script:</p>
<div class="highlight"><pre><span></span><span class="c1"># app.py (v2 - final version)</span>

<span class="sd">&quot;&quot;&quot;Script to crawl Top Posts across sub reddits and store results in MongoDB</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">date</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>
<span class="kn">from</span> <span class="nn">reddit.spiders</span> <span class="kn">import</span> <span class="n">PostSpider</span>
<span class="kn">from</span> <span class="nn">top_post_emailer</span> <span class="kn">import</span> <span class="n">email_last_scraped_date</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="c1"># only run on saturdays (once a week)</span>
    <span class="k">if</span> <span class="n">date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">(),</span> <span class="s1">&#39;%A&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;saturday&#39;</span><span class="p">:</span>
        <span class="n">crawler</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">get_project_settings</span><span class="p">())</span>

        <span class="n">crawler</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">PostSpider</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># the script will block here until the crawling is finished</span>

        <span class="n">email_last_scraped_date</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Scrape complete and email sent.&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Script did not run.&#39;</span><span class="p">)</span>
</pre></div>


<h5>Note</h5>
<ul>
<li>We used the <a href="https://docs.python.org/3/library/logging.html">Python <code>logging</code> module</a> to provide additional details on the innerworkings of our application. Another fantastic tool available in the <a href="https://docs.python.org/3/library/index.html">Python Standard Library</a>. <em>Batteries are included</em></li>
</ul>
<hr>
<h2>PythonAnywhere</h2>
<p>In this section we will explore the PythonAnywhere cloud platform, set up a Python virtual environment for our scraper, and configure the PythonAnywhere scheduler to run our Scrapy script.</p>
<h3>Overview</h3>
<p><a href="https://www.pythonanywhere.com/">From the website</a>:</p>
<blockquote>
<p>PythonAnywhere makes it easy to create and run Python programs in the cloud. You can write your programs in a web-based editor or just run a console session from any modern web browser. There's storage space on our servers, and you can preserve your session state and access it from anywhere, with no need to pay for, or configure, your own server. Start work on your work desktop, then later pick up from where you left off by accessing exactly the same session from your laptop.</p>
</blockquote>
<p><em>What does this mean?</em> For 5 bucks a month, we can host websites and schedule batch jobs in a fully configurable Python environment.</p>
<p>More information can be found on <a href="https://www.pythonanywhere.com/">their website</a> and <a href="https://talkpython.fm/episodes/show/10/bringing-python-to-the-masses-with-hosting-and-devops-at-pythonanywhere">Episode 10 of the Talk Python to Me podcast</a>.</p>
<h3>Setting Up Environment</h3>
<p>After you create an account, you will need to open up a Bash console:<br />
<img src="/images/1-10/5_pythonanywhere_console.png" alt="PythonAnywhere - Create Console" width=400/></p>
<h4><code>virtualenv</code> and <code>pip</code></h4>
<p>Now we will set up a <code>virtualenv</code> and <code>pip install</code> required packages:</p>
<div class="highlight"><pre><span></span><span class="gp">11:06 ~ $</span> mkvirtualenv scrapy36 --python<span class="o">=</span>/usr/bin/python3.6
<span class="go">Running virtualenv with interpreter /usr/bin/python3.6</span>
<span class="go">Using base prefix &#39;/usr&#39;</span>
<span class="go">New python executable in /home/pythonsivji/.virtualenvs/scrapy36/bin/python3.6</span>
<span class="go">Also creating executable in /home/pythonsivji/.virtualenvs/scrapy36/bin/python</span>
<span class="go">Installing setuptools, pip, wheel...done.</span>
<span class="go">virtualenvwrapper.user_scripts creating /home/pythonsivji/.virtualenvs/scrapy36/bin/predeactivate</span>
<span class="go">virtualenvwrapper.user_scripts creating /home/pythonsivji/.virtualenvs/scrapy36/bin/postdeactivate</span>
<span class="go">virtualenvwrapper.user_scripts creating /home/pythonsivji/.virtualenvs/scrapy36/bin/preactivate</span>
<span class="go">virtualenvwrapper.user_scripts creating /home/pythonsivji/.virtualenvs/scrapy36/bin/postactivate</span>
<span class="go">virtualenvwrapper.user_scripts creating /home/pythonsivji/.virtualenvs/scrapy36/bin/get_env_details</span>
<span class="go">(scrapy36) 11:08 ~ $ which python</span>
<span class="go">/home/pythonsivji/.virtualenvs/scrapy36/bin/python</span>
<span class="go">(scrapy36) 11:08 ~ $ pip install scrapy mongoengine jinja2 requests</span>
<span class="go">Collecting scrapy</span>
<span class="go">  Downloading Scrapy-1.3.3-py2.py3-none-any.whl (240kB)</span>
<span class="go">    100% |████████████████████████████████| 245kB 2.2MB/s</span>
<span class="go">Collecting mongoengine</span>
<span class="go">  Downloading mongoengine-0.11.0.tar.gz (352kB)</span>
<span class="go">    100% |████████████████████████████████| 358kB 1.8MB/s</span>
<span class="go">Collecting jinja2</span>
<span class="go">Collecting requests</span>
<span class="go">... additional rows ommitted ...</span>
</pre></div>


<h5>Additional Resources</h5>
<ul>
<li><a href="https://realpython.com/blog/python/python-virtual-environments-a-primer/">Python Virtual Environments - a Primer</a> - Real Python</li>
<li><a href="https://help.pythonanywhere.com/pages/Virtualenvs/">PythonAnywhere help page: <code>virtualenv</code></a></li>
</ul>
<h4>Uploading code into PythonAnywhere</h4>
<p>There are <a href="https://help.pythonanywhere.com/pages/FTP/">many ways</a> to get code in and out of PythonAnywhere. We will be uploading our local project folder to Github and then cloning our repo into PythonAnywhere as follows:</p>
<div class="highlight"><pre><span></span><span class="go">(scrapy36) 23:50 ~ $ mkdir siv-dev &amp;&amp; cd siv-dev</span>
<span class="go">(scrapy36) 23:51 ~/siv-dev $ git clone https://github.com/alysivji/reddit-top-posts-scrapy.git</span>
<span class="go">Cloning into &#39;reddit-top-posts-scrapy&#39;...</span>
<span class="go">remote: Counting objects: 63, done.</span>
<span class="go">remote: Compressing objects: 100% (23/23), done.</span>
<span class="go">remote: Total 63 (delta 9), reused 0 (delta 0), pack-reused 39</span>
<span class="go">Unpacking objects: 100% (63/63), done.</span>
<span class="go">Checking connectivity... done.</span>
</pre></div>


<h5>Notes</h5>
<ul>
<li>Creating a git repo is not covered here. There are plenty of online resources available that cover this topic in the requisite detail. <a href="https://xkcd.com/1597/">Relevant xkcd</a></li>
<li>Make sure you add <code>settings.cfg</code> to your <code>.gitignore</code> file if you are using a public repo. In this case, you will need to manually upload the <code>settings.cfg</code> files into their respective folders</li>
</ul>
<h3>Set up Scheduler</h3>
<p>In order to <a href="https://help.pythonanywhere.com/pages/VirtualEnvInScheduledTasks">run a scheduled task in a virtualenv</a>, we require the full path to the virtualenv's Python. We can get this as follows:</p>
<div class="highlight"><pre><span></span><span class="go">(scrapy36) 23:52 ~/siv-dev/reddit-top-posts-scrapy (master)$ which python</span>
<span class="go">/home/pythonsivji/.virtualenvs/scrapy36/bin/python</span>
<span class="go">(scrapy36) 23:54 ~/siv-dev/reddit-top-posts-scrapy (master)$ pwd</span>
<span class="go">/home/pythonsivji/siv-dev/reddit-top-posts-scrapy</span>
</pre></div>


<p>Now we will create a bash script to <code>cd</code> into our project directory and execute the Scrapy script:</p>
<div class="highlight"><pre><span></span><span class="c1"># ./run_reddit_scraper</span>

<span class="c1">#!/bin/bash</span>
<span class="c1"># scrapy project files require us to be in the project folder to load settings</span>
<span class="c1"># run with scrapy virtualenv</span>
<span class="nb">cd</span> /home/pythonsivji/siv-dev/reddit-top-posts-scrapy/
<span class="s2">&quot;/home/pythonsivji/.virtualenvs/scrapy36/bin/python&quot;</span> app.py
</pre></div>


<p>In the console, we will need to change permissions to make our newly-created file executable:</p>
<div class="highlight"><pre><span></span><span class="gp">$</span> chmod <span class="m">755</span> run_reddit_scraper
</pre></div>


<p>Next, we schedule the following command to run <strong>Daily</strong> at <strong>10:00 UTC</strong>:</p>
<div class="highlight"><pre><span></span><span class="go">./run_reddit_scraper</span>
</pre></div>


<p>The Schedule dashboard should look as follows:<br />
<img src="/images/1-10/5_python_anywhere_schedule_task.png" alt="PythonAnywhere - Scheduled Task" width=400/></p>
<h3>Monitoring Runtime</h3>
<p>Let's take a look at the log to make sure everything is working:</p>
<div class="highlight"><pre><span></span><span class="go">... additional rows omitted ...</span>
<span class="go">DEBUG:root:Post added to MongoDB</span>
<span class="go">2017-04-01 14:45:29 [root] DEBUG: Post added to MongoDB</span>
<span class="go">DEBUG:scrapy.core.scraper:Scraped from &lt;200 https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=week&gt;</span>
<span class="go">{&#39;commentsUrl&#39;: &#39;https://www.reddit.com/r/MachineLearning/comments/61kym3/p_poker_hand_classification_advice_needed/&#39;,</span>
<span class="go"> &#39;date&#39;: datetime.datetime(2017, 4, 1, 14, 45, 29, 385712),</span>
<span class="go"> &#39;date_str&#39;: &#39;2017-04-01&#39;,</span>
<span class="go"> &#39;score&#39;: 24,</span>
<span class="go"> &#39;sub&#39;: &#39;MachineLearning&#39;,</span>
<span class="go"> &#39;title&#39;: &#39;[P] Poker hand classification, advice needed&#39;,</span>
<span class="go"> &#39;url&#39;: &#39;https://www.reddit.com/r/MachineLearning/comments/61kym3/p_poker_hand_classification_advice_needed/&#39;}</span>
<span class="go">INFO:scrapy.core.engine:Closing spider (finished)</span>
<span class="go">2017-04-01 14:45:29 [scrapy.core.engine] INFO: Closing spider (finished)</span>
<span class="go">2017-04-01 14:45:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span>
<span class="go">{&#39;downloader/request_bytes&#39;: 2388,</span>
<span class="go"> &#39;downloader/request_count&#39;: 7,</span>
<span class="go"> &#39;downloader/request_method_count/GET&#39;: 7,</span>
<span class="go"> &#39;downloader/response_bytes&#39;: 101355,</span>
<span class="go"> &#39;downloader/response_count&#39;: 7,</span>
<span class="go"> &#39;downloader/response_status_count/200&#39;: 5,</span>
<span class="go"> &#39;downloader/response_status_count/301&#39;: 2,</span>
<span class="go"> &#39;finish_reason&#39;: &#39;finished&#39;,</span>
<span class="go"> &#39;finish_time&#39;: datetime.datetime(2017, 4, 1, 14, 45, 29, 398366),</span>
<span class="go"> &#39;item_scraped_count&#39;: 100,</span>
<span class="go"> &#39;log_count/DEBUG&#39;: 208,</span>
<span class="go"> &#39;log_count/INFO&#39;: 7,</span>
<span class="go"> &#39;response_received_count&#39;: 5,</span>
<span class="go"> &#39;scheduler/dequeued&#39;: 6,</span>
<span class="go"> &#39;scheduler/dequeued/memory&#39;: 6,</span>
<span class="go"> &#39;scheduler/enqueued&#39;: 6,</span>
<span class="go"> &#39;scheduler/enqueued/memory&#39;: 6,</span>
<span class="go"> &#39;start_time&#39;: datetime.datetime(2017, 4, 1, 14, 45, 26, 699177)}</span>
<span class="go">INFO:scrapy.core.engine:Spider closed (finished)</span>
<span class="go">2017-04-01 14:45:29 [scrapy.core.engine] INFO: Spider closed (finished)</span>
<span class="go">DEBUG:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): api.mailgun.net</span>
<span class="go">2017-04-01 14:45:29 [requests.packages.urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): api.mailgun.net</span>
<span class="go">DEBUG:requests.packages.urllib3.connectionpool:https://api.mailgun.net:443 &quot;POST /v3/sandbox024e4bcae7814311932f6e9569cea611.mailgun.org/messages HTTP/1.1&quot; 200 138</span>
<span class="go">2017-04-01 14:45:30 [requests.packages.urllib3.connectionpool] DEBUG: https://api.mailgun.net:443 &quot;POST /v3/sandbox024e4bcae7814311932f6e9569cea611.mailgun.org/messages HTTP/1.1&quot; 200 138</span>
<span class="go">INFO:__main__:Scrape complete and email sent.</span>
<span class="go">2017-04-01 14:45:30 [__main__] INFO: Scrape complete and email sent.</span>
<span class="go">Success!</span>

<span class="go">2017-04-01 14:45:30 -- Completed task, took 11.00 seconds, return code was 0.</span>
</pre></div>


<p>Was the email digest sent?<br />
<img src="/images/1-10/5_email_digest.png" alt="PythonAnywhere - Create Console" width=400/></p>
<p>And we're done!</p>
<hr>
<h2>Conclusion</h2>
<p>In this post, we refactored our Reddit Top Posts web scraper to automatically run on the PythonAnywhere cloud. Now that our data collection process is automated, we just have to monitor emails to ensure the script is working as intended.</p>
  </div>
  
<div class="article-tag-list">
<span class="label label-default">Tags</span>
	<a href="./tag/python.html"><i class="fa fa-tag"></i>python</a>&nbsp;
	<a href="./tag/how-to.html"><i class="fa fa-tag"></i>how-to</a>&nbsp;
	<a href="./tag/mongodb.html"><i class="fa fa-tag"></i>mongodb</a>&nbsp;
	<a href="./tag/web-scraping.html"><i class="fa fa-tag"></i>web-scraping</a>&nbsp;
	<a href="./tag/reddit-scraper.html"><i class="fa fa-tag"></i>reddit-scraper</a>&nbsp;
</div>  <hr />
  <div class="well well-sm">  <!-- Social media sharing buttons -->
    <!-- Twitter -->
    <a href="https://twitter.com/share" class="twitter-share-button" 
       data-via="CaiusSivjus" >Tweet</a>&nbsp;

    <!-- Google+ -->
    <div class="g-plus" data-action="share" data-annotation="bubble"></div>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <div class="g-plusone" data-size="medium"></div>&nbsp;

    <!-- Facebook -->
    <div class="fb-like" 
        data-href="./scrapy-part2-running-scrapy-on-pythonanywhere.html" 
        data-layout="button_count" 
        data-action="like" data-show-faces="true" 
        data-share="true">
    </div>
    &nbsp;
  </div> <!-- /Social media sharing buttons -->
  <div class="comments">
	<h2>Comments</h2>
	<div id="disqus_thread"></div>
	<script type="text/javascript">
				   (function() {
						var dsq = document.createElement('script');
						dsq.type = 'text/javascript'; dsq.async = true;
						dsq.src = '//siv-scripts.disqus.com/embed.js';
						(document.getElementsByTagName('head')[0] ||
						 document.getElementsByTagName('body')[0]).appendChild(dsq);
				  })();
	</script>
  </div>
</article>
        </div><!-- /content -->

        <div class="col-md-3 sidebar-nav" id="sidebar">

<div class="row">

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-comment fa-fw fa-lg"></i> Social</h4>
<ul class="list-unstyled social-links">
    <li><a href="http://linkedin.com/in/alysivji/" target="_blank">
	  <i class="fa fa-linkedin-square fa-fw fa-lg" title="LinkedIn"></i>
		LinkedIn
	</a></li>
    <li><a href="http://github.com/alysivji" target="_blank">
	  <i class="fa fa-github-square fa-fw fa-lg" title="GitHub"></i>
		GitHub
	</a></li>
    <li><a href="https://twitter.com/CaiusSivjus" target="_blank">
	  <i class="fa fa-twitter-square fa-fw fa-lg" title="Twitter"></i>
		Twitter
	</a></li>
</ul>
</div>

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-folder fa-fw fa-lg"></i> Categories</h4>
<ul class="list-unstyled category-links">
  <li><a href="./category/data-analysis.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Data Analysis</a></li>
  <li><a href="./category/deep-dives.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Deep Dives</a></li>
  <li><a href="./category/quick-hits.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Quick Hits</a></li>
  <li><a href="./category/thoughts.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Thoughts</a></li>
  <li><a href="./category/tutorials.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Tutorials</a></li>
</ul>
</div>

</div> <!-- /row -->

<h4><i class="fa fa-tags fa-fw fa-lg"></i> Tags</h4>
<p class="tag-cloud">
    <span class="tag-4">
      <a href="./tag/web-development.html">
          <i class="fa fa-tag"></i>
        web-development
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/exploring-pypi.html">
          <i class="fa fa-tag"></i>
        exploring-pypi
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/vim.html">
          <i class="fa fa-tag"></i>
        vim
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/terminal.html">
          <i class="fa fa-tag"></i>
        terminal
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/flask.html">
          <i class="fa fa-tag"></i>
        flask
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/data-science.html">
          <i class="fa fa-tag"></i>
        data-science
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/review.html">
          <i class="fa fa-tag"></i>
        review
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/testing.html">
          <i class="fa fa-tag"></i>
        testing
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/gis.html">
          <i class="fa fa-tag"></i>
        gis
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/data-viz.html">
          <i class="fa fa-tag"></i>
        data-viz
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/book.html">
          <i class="fa fa-tag"></i>
        book
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/cest-la-vie.html">
          <i class="fa fa-tag"></i>
        c'est-la-vie
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/reddit-scraper.html">
          <i class="fa fa-tag"></i>
        reddit-scraper
      </a>
    </span>
    <span class="tag-1">
      <a href="./tag/python.html">
          <i class="fa fa-tag"></i>
        python
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/grad-school.html">
          <i class="fa fa-tag"></i>
        grad-school
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/workflow.html">
          <i class="fa fa-tag"></i>
        workflow
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/productivity.html">
          <i class="fa fa-tag"></i>
        productivity
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/mongodb.html">
          <i class="fa fa-tag"></i>
        mongodb
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/pythonic.html">
          <i class="fa fa-tag"></i>
        pythonic
      </a>
    </span>
    <span class="tag-3">
      <a href="./tag/web-scraping.html">
          <i class="fa fa-tag"></i>
        web-scraping
      </a>
    </span>
    <span class="tag-2">
      <a href="./tag/pandas.html">
          <i class="fa fa-tag"></i>
        pandas
      </a>
    </span>
    <span class="tag-1">
      <a href="./tag/how-to.html">
          <i class="fa fa-tag"></i>
        how-to
      </a>
    </span>
    <span class="tag-4">
      <a href="./tag/docker.html">
          <i class="fa fa-tag"></i>
        docker
      </a>
    </span>
</p>
<h4><i class="fa fa-rss fa-fw fa-lg"></i> Feeds</h4>
<ul class="list-unstyled">
    <li><a href="http://alysivji.github.io/feeds/all.atom.xml" 
		   type="application/atom+xml" rel="alternate">
		<i class="fa fa-rss-square fa-fw fa-lg"></i> Atom Feed</a></li>
</ul>

<hr />

        </div><!--/sidebar -->
      </div><!--/row-->
    </div><!--/.container /#main-container -->

    <footer id="site-footer">
 
      <address id="site-colophon">
        <p class="text-center text-muted">
        Site built using <a href="http://getpelican.com/" target="_blank">Pelican</a>
        &nbsp;&bull;&nbsp; Theme based on
        <a href="http://www.voidynullness.net/page/voidy-bootstrap-pelican-theme/"
           target="_blank">VoidyBootstrap</a> by 
        <a href="http://www.robertiwancz.com/"
           target="_blank">RKI</a>  
        </p>
      </address><!-- /colophon  -->
    </footer>

<!-- DISQUS script for displaying comment count -->
<script type="text/javascript">
    var disqus_shortname = 'siv-scripts';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>

    <!-- javascript -->
   
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
            integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
            crossorigin="anonymous"></script>


<!-- Facebook -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script>

<!-- Twitter -->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<!-- Google+ -->
<!-- Synchronous 
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
-->
<!-- Asynchronous -->
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/platform.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>  </body>
</html>